# Open targets internal deployment guide

## Overview

Using project: `open-targets-genetics-dev`.

Currently the genetics team provides input files in a GCP bucket `gs://genetics-portal-dev-staging` (`staging`). Some of
these files are static, others are annotated with a date (variously YYMMDD and DDMMYY).

A _subset_ of these files are then _manually_ copied by the BE team to `gs://genetics-portal-dev-data` (`dev`) in a
bucket corresponding to the release.

The files in `dev` are used to run the pipeline, typically using Dataproc.

| Configuration field | Likely staging location | Standard dev location |
| --- | --- | --- |
| `variant-index.raw` | *provided by data team* | /variant-annotation/<date>/variant-annotation.parquet |
| `ensembl.lut` | *generated by BE* | /lut/homo_sapiens_core_105_38_genes.json.gz |
| `vep.homo-sapiens-cons-scores` | *should be in staging bucket* | /lut/vep_consequences.tsv |
| `interval.path` |v2g/interval/\* | /v2g/interval/\*/\*/<date>/data.parquet |
| `qtl.path` | v2g/qtl/\<date\>/ | v2g/qlt/\<date\> |
| `variant-gene.weights` | *carried over from previous release* | lut/v2g_scoring_source_weights.date.json |
| `variant-disease.studies` | v2d/\<date\>/studies.parquet | v2d/studies.parquet |
| `variant-disease.toploci` | v2d/\<date\>/toploci.parquet | v2d/toploci.parquet|
| `variant-disease.finemapping` | v2d/\<date\>/finemapping.parquet/ | v2d/finemapping.parquet |
| `variant-disease.ld` | v2d/\<date\>/ld.parquet/ | v2d/ld.parquet |
| `variant-disease.overlapping` | v2d/\<date\>/locus_overlap.parquet | v2d/locus_overlap.parquet |
| `variant-disease.coloc` | coloc/\<date\>/coloc_processed_w_betas.parquet/ | v2d/coloc_processed_w_betas.parquet |
| `variant-disease.trait_efo` | v2d/\<date\>/trait_efo-2021-11-16.parquet | v2d/trait_efo.parquet |

### Variant index section

The variant index comes in parquet _from the data team_ after filtering the latest Gnomad release.

If there is no new update keep using the last one used. Currently, the variant annotation is version 190129.

### Ensembl

Genetics team do not provide the Ensembl file: we have to download it ourselves and generate the input.

It is a configuration place to bring the latest reference gene table from Ensembl. To generate this file to need to
follow the instructions [from this script](https://github.com/opentargets/genetics-backend/tree/master/makeLUTs). And
the command I use is this as an example

```python create_genes_dictionary.py -o "./" -z -n homo_sapiens_core_104_38```

### VEP consequences

The TSV file is provided by the _genetics team_. If the file is not present in the staging bucket ask the Genetics team
for the most recent version.

### Recipe: set up deployment machine

We need a VM to run deployments from. Typically this only needs to be done once and then we can use the machine for
future releases.

```bash
# install dependencies
sudo apt install -y git \
tmux tree wget \
libgl1-mesa-glx libegl1-mesa libxrandr2 libxrandr2 libxss1 libxcursor1 libxcomposite1 libasound2 libxi6 libxtst6

wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O ~/miniconda.sh
bash ~/miniconda.sh -p $HOME/miniconda
source ~/.bashrc

# get repositories
git clone https://github.com/opentargets/genetics-backend.git

# set up conda environments
cd genetics-backend && conda env create -f environment.yaml

```

### Recipe: get all inputs

Use the VM in the `open-target-genetics-dev` machine called `gp-deploy`. The VM is preconfigured with the necessary
utilities to run a release.

- [ ] clone required repository: `git clone git@github.com:opentargets/genetics-backend.git`
- [ ] set up environment: `conda activate backend-genetics`
- [ ] update Ensembl version and run script: `python create_genes_dictionary.py -o "./" -z -n homo_sapiens_core_105_38`
- [ ] add ensembl file to bucket `gsutil cp -n homo_sapiens* gs://genetics-portal-dev-data/22.01/inputs/lut/`
- [ ] update variables in bash script in `/scripts/prepare_inputs.sh` (input script)
- [ ] run input script in VM to move files from staging to dev buckets
    - Most of the inputs are used for the pipeline, but there are two static datasets which are copied, sumstats (sa)
      and `v2g_credset`.
- [ ] create a configuration file for release in `config`:
    - [ ] `cp src/main/resources/application.conf config/<release>.conf` and update as necessary.
- [ ] update top level variables in `scripts/run_cluster.sh`: `release` and `config` should be the only changes 
  necessary. 
- [ ] run script `scripts/run_cluster.sh` from root directory. This script builds a jar file, pushes it to GS 
  storage, starts a cluster and runs all steps. 
  
Once this is done, the genetics team needs to run the `l2g` ML pipeline to create the last input necessary for the 
`manhattan` step.